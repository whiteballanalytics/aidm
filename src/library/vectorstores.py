import json, io, time, hashlib
from pathlib import Path
from pydantic import BaseModel, PrivateAttr
from agents import FileSearchTool
from openai import OpenAI
from typing import Optional


# Local-machine specific paths to vector stores
CONFIG_PATH = Path("config/vectorstores.json")        # Store for e.g. world lore
MEM_REGISTRY_PATH = Path("config/memorystores.json")  # Store for campaign memory


def add_to_vector_store(category: str, store_name: str, id: str):
    """
    Add or update a vector store entry in the config file.
    
    Args:
        category: Top-level namespace (e.g., "world", "tools")
        store_name: Human-readable key under the category (e.g., "Fiction")
        id: The vector store identifier (e.g., "vs_abc123...")
    
    Example:
        add_to_vector_store("world", "Fiction", "vs_6898...be73")
    """

    CONFIG_PATH.parent.mkdir(parents=True, exist_ok=True)
    data = {}
    if CONFIG_PATH.exists():
        try:
            data = json.loads(CONFIG_PATH.read_text())
        except json.JSONDecodeError:
            data = {}

    data.setdefault(category, {})  # Creates (if missing) a top-level category object (e.g., "world" for world lore)
    data[category][store_name] = {"vector_store_id": id}

    # Writes the updated JSON back to disk
    CONFIG_PATH.write_text(json.dumps(data, indent=2), encoding="utf-8")


class LoreSearch(BaseModel):
    """
    Thin facade for a FileSearchTool that: 
      1) reads your config file,
      2) exposes a preconfigured FileSearchTool for the DM agent,
      3) (optionally) lets you tune max results / filters.
    """

    vector_store_id: str
    max_num_results: int = 8  # How many chunks/snippets to bring back per search
    include_search_results: bool = True  # let the model see snippets it retrieved

    @classmethod
    def set_lore(cls, collection: str, domain: str = "world") -> "LoreSearch":
        """
        Look up the vector store ID in config/vectorstores.json under domain/collection
        and return a LoreSearch configured for that vector store.
        """
        data = json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
        try:
            vs_id = data[domain][collection]["vector_store_id"]
        except KeyError as e:
            raise KeyError(
                f"Could not find vector store id at path [{domain}][{collection}][vector_store_id] "
                f"in {CONFIG_PATH}"
            ) from e
        return cls(vector_store_id=vs_id)

    def as_tool(self) -> FileSearchTool:
        """
        Return an Agents SDK FileSearchTool configured to query *this* vector store.
        A senior agent will call this tool when it needs canon.
        """
        return FileSearchTool(
            vector_store_ids=[self.vector_store_id],
            max_num_results=self.max_num_results,
            include_search_results=self.include_search_results,
        )


def get_campaign_mem_store(client: OpenAI, campaign_id: str) -> str:
    """
    Create or load the vector store id a campaign and cache it locally.
    
    Args:
        client: An initialized OpenAI client
        campaign_id: The logical identifier for a campaign
    
    Returns:
        The vector store ID string (e.g., "vs_abc123...")
    
    Typical usage:
        client = OpenAI(api_key=AGENT_KEY)
        mem_store_id = get_campaign_mem_store(client, CAMPAIGN_ID)
    """

    if MEM_REGISTRY_PATH.exists():
        try:
            reg = json.loads(MEM_REGISTRY_PATH.read_text(encoding="utf-8"))
        except Exception:
            reg = {}
    else:
        reg = {}

    if campaign_id in reg:
        return reg[campaign_id]

    vs = client.vector_stores.create(name=f"mem_{campaign_id}")
    reg[campaign_id] = vs.id
    MEM_REGISTRY_PATH.parent.mkdir(parents=True, exist_ok=True)
    MEM_REGISTRY_PATH.write_text(json.dumps(reg, ensure_ascii=False, indent=2), encoding="utf-8")

    return vs.id
    

class MemorySearch(BaseModel):
    """
    Per-campaign long-term memory:
      1) ensures/loads the campaign vector store id,
      2) exposes a FileSearchTool for the DM agent,
      3) can append 'memory_writes' cheaply (upload+attach JSON).

    Memories are generated by the agent with every in-game event.
    These are then stored so that the game will have internal consistency.
    """

    campaign_id: str  # Logical name for the campaign this memory belongs to
    vector_store_id: str  # The actual OpenAI vector store id
    max_num_results: int = 12  # How many memory chunks to retrieve per call
    include_search_results: bool = True  # let the model see snippets it retrieved
    
    _client: OpenAI = PrivateAttr(default_factory=OpenAI)  # The OpenAI client instance used
    _mirror_dir: Optional[Path] = PrivateAttr(default=None)  # Local mirror for human inspection

    # --- constructors ---
    @classmethod
    def from_id(cls, campaign_id: str, vector_store_id: str, client: Optional[OpenAI] = None) -> "MemorySearch":
        """
        Build a MemorySearch bound to a known store id.
        If client is not provided, it creates one from env.
        """
        inst = cls(campaign_id=campaign_id, vector_store_id=vector_store_id)
        inst._client = client or OpenAI()
        return inst
    
    # --- tool exposure ---
    def as_tool(self) -> FileSearchTool:
        """
        Return an Agents SDK FileSearchTool configured to query this campaign's memory store.
        """
        return FileSearchTool(
            vector_store_ids=[self.vector_store_id],
            max_num_results=self.max_num_results,
            include_search_results=self.include_search_results,
        )

    # --- append memory writes (no LLM tokens) ---
    def with_mirror(self, path: str | Path) -> "MemorySearch":
        """
        Enables a folder where every memory write is also saved as .json.
        Workaround for “assistants” files not being downloadable from the API.
        """
        p = Path(path)
        p.mkdir(parents=True, exist_ok=True)
        self._mirror_dir = p
        return self
    
    def upsert_memory_writes(self, user_id: str, memory_writes: list[dict]) -> Optional[str]:
        """
        Takes a memory_writes array (type/keys/summary) from an agent,
        wraps it in a tiny JSON payload, uploads it as a file, and attaches it to the vector store.
        If a mirror is configured, it also writes the same JSON to disk so you can open it later.
        """
        if not memory_writes:
            return None

        # Prepare the payload
        payload = {
            "campaign_id": self.campaign_id,
            "user_id": user_id,
            "items": memory_writes,
            "ts": int(time.time()),
        }
        raw = json.dumps(payload, ensure_ascii=False, indent=2)

        # digest = short fingerprint used in the filename to reduce the chance of collisions
        digest = hashlib.sha1(raw.encode("utf-8")).hexdigest()[:10]
        fname = f"mem_{self.campaign_id}_{int(time.time())}_{digest}.json"

        # Builds an in-memory file object for upload.
        # Setting name helps the API record the filename metadata
        # (so you’ll see it in dashboards/lists)
        buf = io.BytesIO(raw.encode("utf-8"))
        buf.name = fname

        # Upload to vector store (for retrieval by the model)
        f = self._client.files.create(file=buf, purpose="assistants")
        self._client.vector_stores.files.create(vector_store_id=self.vector_store_id, file_id=f.id)

        # Mirror locally so you can read later
        if self._mirror_dir:
            (self._mirror_dir / fname).write_text(raw, encoding="utf-8")

        return f.id